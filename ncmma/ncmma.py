#!/usr/bin/env python3
"""
CMMA API ä¾¡æ ¼ç›£è¦–ãƒãƒƒãƒã‚·ã‚¹ãƒ†ãƒ 
- é‡è¤‡æŠ•ç¨¿é˜²æ­¢æ©Ÿèƒ½ (SQLite)
- ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•å‰Šé™¤æ©Ÿèƒ½
- æ™‚é–“è¶³ã«å¿œã˜ãŸå†é€šçŸ¥åˆ¶å¾¡
"""

import requests
import json
import time
import sys
import os
import hashlib
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from dotenv import load_dotenv
import logging
from logging.handlers import RotatingFileHandler

class CmmaPriceMonitor:
    def __init__(self, config_path=None):
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’åŸºæº–ã«ãƒ‘ã‚¹ã‚’è¨­å®š
        self.script_dir = Path(__file__).parent.absolute()
        self.config_path = config_path or self.script_dir / '.env'
        self.log_dir = self.script_dir / 'logs'
        self.cache_dir = self.script_dir / 'cache'
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
        self.log_dir.mkdir(exist_ok=True)
        self.cache_dir.mkdir(exist_ok=True)
        
        # è¨­å®šã®èª­ã¿è¾¼ã¿ã¨ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
        self._load_config()
        self._setup_logging()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
        self.data_dir = self.script_dir / 'data'
        self.data_dir.mkdir(exist_ok=True)
        self.db_path = self.data_dir / 'ncmma.db'
        self._init_db()

        if not self.discord_webhook_url:
            raise ValueError("DISCORD_WEBHOOK_URL not found in environment variables")
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        if self.config_path.exists():
            load_dotenv(self.config_path)
        else:
            load_dotenv()
        
        self.discord_webhook_url = os.getenv('DISCORD_WEBHOOK_URL')
        self.volatility_api_url = os.getenv('CMMA_VOLATILITY_API_URL', 'https://stg.api.1btc.love/volatility')
        
        # APIã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.timeframe = os.getenv('TIMEFRAME', '4h')
        self.threshold = float(os.getenv('THRESHOLD', '5.0'))
        self.direction = os.getenv('DIRECTION', 'up')
        self.sort = os.getenv('SORT', 'volatility_desc')
        self.limit = int(os.getenv('LIMIT', '100'))
        self.offset = int(os.getenv('OFFSET', '5'))

        # ç›£è¦–è¨­å®š
        self.max_notifications = int(os.getenv('MAX_NOTIFICATIONS', '20'))
        self.renotify_buffer_minutes = int(os.getenv('RENOTIFY_BUFFER_MINUTES', '60'))
        self.check_interval_seconds = int(os.getenv('CHECK_INTERVAL_SECONDS', '300'))
        
        # ãƒ­ã‚°è¨­å®š
        self.log_max_size_mb = int(os.getenv('LOG_MAX_SIZE_MB', '10'))

        # å‡ºæ¥é«˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼è¨­å®š
        self.volume_api_url = os.getenv('CMMA_VOLUME_API_URL', 'https://stg.api.1btc.love/volume')
        self.volume_threshold = float(os.getenv('VOLUME_THRESHOLD', '0.0'))
        self.volume_timeframe = os.getenv('VOLUME_TIMEFRAME', '1h')
        self.volume_period = os.getenv('VOLUME_PERIOD', '24h')

    def _init_db(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS notifications (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        notification_hash TEXT UNIQUE NOT NULL,
                        symbol TEXT NOT NULL,
                        timeframe TEXT NOT NULL,
                        direction TEXT NOT NULL,
                        change_pct REAL NOT NULL,
                        notified_at TIMESTAMP NOT NULL
                    )
                """)
                conn.commit()
        except sqlite3.Error as e:
            self.logger.error(f"Database initialization failed: {e}")
            raise

    def _setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        log_file = self.log_dir / 'ncmma_monitor.log'
        
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãï¼‰
        file_handler = RotatingFileHandler(
            log_file, maxBytes=5*1024*1024, backupCount=3
        )
        file_handler.setFormatter(formatter)
        file_handler.setLevel(logging.INFO)
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        console_handler.setLevel(logging.INFO)
        
        # ãƒ­ã‚¬ãƒ¼è¨­å®š
        self.logger = logging.getLogger('CmmaMonitor')
        self.logger.setLevel(logging.INFO)
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        self.logger.propagate = False
    
    def _cleanup_logs(self):
        """ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è‡ªå‹•å‰Šé™¤ï¼ˆ10MBä»¥ä¸Šï¼‰"""
        try:
            total_size = 0
            log_files = []
            
            # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
            for file_path in self.log_dir.rglob('*'):
                if file_path.is_file():
                    size = file_path.stat().st_size
                    total_size += size
                    log_files.append((file_path, size, file_path.stat().st_mtime))
            
            total_size_mb = total_size / (1024 * 1024)
            self.logger.info(f"Total log directory size: {total_size_mb:.2f}MB")
            
            # 10MBä»¥ä¸Šã®å ´åˆã€å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‰Šé™¤
            if total_size_mb > self.log_max_size_mb:
                self.logger.info(f"Log directory exceeds {self.log_max_size_mb}MB, cleaning up...")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆå¤ã„é †ï¼‰
                log_files.sort(key=lambda x: x[2])
                
                deleted_size = 0
                deleted_count = 0
                
                for file_path, size, mtime in log_files:
                    # ç¾åœ¨ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¨ãƒ¡ã‚¤ãƒ³ãƒ­ã‚°ã¯å‰Šé™¤ã—ãªã„
                    if (file_path.name == 'ncmma_monitor.log' or 
                        file_path.name.startswith('ncmma_monitor.log.')):
                        continue
                    
                    try:
                        file_path.unlink()
                        deleted_size += size
                        deleted_count += 1
                        self.logger.info(f"Deleted old log file: {file_path.name}")
                        
                        # ç›®æ¨™ã‚µã‚¤ã‚ºä»¥ä¸‹ã«ãªã£ãŸã‚‰åœæ­¢
                        if (total_size - deleted_size) / (1024 * 1024) <= self.log_max_size_mb * 0.8:
                            break
                            
                    except Exception as e:
                        self.logger.warning(f"Failed to delete {file_path.name}: {e}")
                
                self.logger.info(f"Cleanup completed: {deleted_count} files deleted, {deleted_size/(1024*1024):.2f}MB freed")
        
        except Exception as e:
            self.logger.error(f"Error during log cleanup: {e}")
    
    
    def _generate_notification_hash(self, symbol, direction):
        """ãƒˆãƒ¼ã‚¯ãƒ³ã®é€šçŸ¥ç”¨ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        # ã‚·ãƒ³ãƒœãƒ« + å¤‰å‹•æ–¹å‘ + æ™‚é–“è¶³ã§ãƒãƒƒã‚·ãƒ¥åŒ–
        hash_input = f"{symbol}_{direction}_{self.timeframe}"
        return hashlib.md5(hash_input.encode()).hexdigest()
    
    def _should_notify(self, notification_hash):
        """é€šçŸ¥ã™ã¹ãã‹ã©ã†ã‹ã‚’DBã§åˆ¤å®š"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT notified_at FROM notifications WHERE notification_hash = ?",
                    (notification_hash,)
                )
                result = cursor.fetchone()

                if result:
                    last_notified_at = datetime.fromisoformat(result[0])
                    time_diff = datetime.now() - last_notified_at
                    
                    if time_diff.total_seconds() < self.renotify_buffer_minutes * 60:
                        remaining_minutes = self.renotify_buffer_minutes - (time_diff.total_seconds() / 60)
                        # self.logger.debug(f"Skipping hash {notification_hash[:8]}...: {remaining_minutes:.1f}min remaining")
                        return False
                return True
        except sqlite3.Error as e:
            self.logger.error(f"Failed to check notification history from DB: {e}")
            return False # DBã‚¨ãƒ©ãƒ¼æ™‚ã¯é€šçŸ¥ã—ãªã„

    def _record_notification(self, notification_hash, symbol, timeframe, direction, change_pct):
        """é€šçŸ¥å±¥æ­´ã‚’DBã«è¨˜éŒ²"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT OR REPLACE INTO notifications (notification_hash, symbol, timeframe, direction, change_pct, notified_at)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (notification_hash, symbol, timeframe, direction, change_pct, datetime.now().isoformat()))
                conn.commit()
        except sqlite3.Error as e:
            self.logger.error(f"Failed to record notification to DB: {e}")

    def fetch_volatility_data(self):
        """CMMA APIã‹ã‚‰ä¾¡æ ¼å¤‰å‹•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        params = {
            'timeframe': self.timeframe,
            'threshold': self.threshold,
            'offset': self.offset,
            'direction': self.direction,
            'sort': self.sort,
            'limit': self.limit,
        }
        try:
            self.logger.info(f"Fetching data from CMMA API with params: {params}")
            response = requests.get(self.volatility_api_url, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            if 'data' in data and data['count'] > 0:
                self.logger.info(f"Successfully fetched {data['count']} records from CMMA API.")
                return data['data']
            elif 'error' in data:
                self.logger.error(f"API Error from CMMA: {data['error']}")
                return []
            else:
                self.logger.info("No significant moves found from CMMA API.")
                return []

        except requests.exceptions.RequestException as e:
            self.logger.error(f"Request error while fetching from CMMA API: {e}")
            return []
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to decode JSON response from CMMA API: {e}")
            return []
        except Exception as e:
            self.logger.error(f"An unexpected error occurred during API fetch: {e}")
            return []

    def fetch_high_volume_data(self):
        """CMMA APIã‹ã‚‰é–¾å€¤ä»¥ä¸Šã®å‡ºæ¥é«˜ã‚’æŒã¤éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if not self.volume_threshold > 0:
            return {}

        params = {
            'timeframe': self.volume_timeframe,
            'period': self.volume_period,
            'min_volume': self.volume_threshold,
            'limit': self.limit,
            'sort': 'volume_desc' # å¿µã®ãŸã‚ã‚½ãƒ¼ãƒˆé †ã‚‚æŒ‡å®š
        }
        try:
            self.logger.info(f"Fetching high volume data from CMMA API with params: {params}")
            response = requests.get(self.volume_api_url, params=params, timeout=30)
            response.raise_for_status()

            data = response.json()
            if 'data' in data:
                self.logger.info(f"Successfully fetched {len(data['data'])} high volume symbols.")
                return {item['symbol']: item for item in data['data']}
            elif 'error' in data:
                self.logger.error(f"API Error from CMMA volume endpoint: {data['error']}")
                return {}
            else:
                self.logger.info("No high volume data found from CMMA API.")
                return {}

        except requests.exceptions.RequestException as e:
            self.logger.error(f"Request error while fetching from CMMA volume API: {e}")
            return {}
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to decode JSON response from CMMA volume API: {e}")
            return {}
        except Exception as e:
            self.logger.error(f"An unexpected error occurred during volume API fetch: {e}")
            return {}
        
    def _format_currency(self, num):
        """æ•°å€¤ã‚’K, M, B, Tã®å˜ä½ã‚’æŒã¤ãƒ‰ãƒ«è¡¨è¨˜æ–‡å­—åˆ—ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹"""
        if num is None:
            return "N/A"
        num = float(num)
        if num < 1000:
            return f"{num:,.2f}$"
        
        magnitude = 0
        while abs(num) >= 1000:
            magnitude += 1
            num /= 1000.0
        
        return f"{num:.2f}{['', 'K', 'M', 'B', 'T'][magnitude]}$"

    def send_discord_notification(self, token_data):
        """Discordã«é€šçŸ¥ã‚’é€ä¿¡ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        if not token_data:
            return False
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’é€šçŸ¥
        filtered_tokens = []
        for token in token_data:
            change_pct = token['change']['pct']
            direction = token['change']['direction']
            notification_hash = self._generate_notification_hash(token['symbol'], direction)
            
            if self._should_notify(notification_hash):
                filtered_tokens.append((token, notification_hash))
        
        if not filtered_tokens:
            self.logger.info("All tokens were filtered out due to recent notifications")
            return False
        
        # æœ€å¤§é€šçŸ¥æ•°ã«åˆ¶é™
        limited_tokens_with_hash = filtered_tokens[:self.max_notifications]
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨è‰²ã‚’å¤‰å‹•æ–¹å‘ã«ã‚ˆã£ã¦å¤‰æ›´
        direction_map = {'up': 'ä¸Šæ˜‡', 'down': 'ä¸‹è½', 'both': 'å¤‰å‹•'}
        title_direction = direction_map.get(self.direction, "å¤‰å‹•")

        title = f"ğŸš€ ä¾¡æ ¼{title_direction}ã‚¢ãƒ©ãƒ¼ãƒˆ"
        color = 0x00ff00 if self.direction == 'up' else (0xff0000 if self.direction == 'down' else 0x0099ff)
            
        description = f"{len(limited_tokens_with_hash)}å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒ{self.timeframe}è¶³ã§{self.threshold}%ä»¥ä¸Šã®{title_direction}ã‚’æ¤œçŸ¥ï¼"

        embed = {
            "title": title,
            "description": description,
            "color": color,
            "timestamp": datetime.utcnow().isoformat(),
            "fields": [],
            "footer": {
                "text": f"ç›£è¦–: CMMA API | é–¾å€¤: {self.threshold}% | æ™‚é–“è¶³: {self.timeframe} | æ–¹å‘: {self.direction}"
            }
        }
        
        # ãƒ•ãƒƒã‚¿ãƒ¼ã«å‡ºæ¥é«˜æ¡ä»¶ã‚’è¿½åŠ 
        if self.volume_threshold > 0:
            formatted_vol_threshold = self._format_currency(self.volume_threshold)
            embed["footer"]["text"] += f" | å‡ºæ¥é«˜({self.volume_period}): >{formatted_vol_threshold}"

        for token, _ in limited_tokens_with_hash:
            change_pct = token['change']['pct']
            direction_char = "ğŸ“ˆ" if token['change']['direction'] == 'up' else "ğŸ“‰"
            sign = "+" if token['change']['direction'] == 'up' else ""

            value = f"**{sign}{change_pct:.2f}%**\n`{token['price']['prev_close']:.6f}` â†’ `{token['price']['close']:.6f}`"
            if 'volume' in token and token['volume'] is not None:
                formatted_volume = self._format_currency(token['volume'])
                value += f"\nVolume: `{formatted_volume}`"

            embed["fields"].append({
                "name": f"{direction_char} {token['symbol']}",
                "value": value,
                "inline": True
            })
        
        # è¡¨ç¤ºåˆ¶é™ã«ã‚ˆã‚‹çœç•¥ãŒã‚ã‚‹å ´åˆ
        if len(filtered_tokens) > self.max_notifications:
            embed["fields"].append({
                "name": "ãã®ä»–",
                "value": f"ã•ã‚‰ã«{len(filtered_tokens) - self.max_notifications}å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ¡ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™...",
                "inline": False
            })
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚‹å ´åˆã®æ³¨è¨˜
        if len(token_data) > len(filtered_tokens):
            skipped_count = len(token_data) - len(filtered_tokens)
            embed["description"] += f"\nï¼ˆ{skipped_count}å€‹ã¯æœ€è¿‘é€šçŸ¥æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰"
        
        payload = {
            "embeds": [embed]
        }
        
        try:
            response = requests.post(self.discord_webhook_url, json=payload, timeout=30)
            response.raise_for_status()
            self.logger.info(f"Discord notification sent: {len(limited_tokens_with_hash)} tokens.")

            # é€šçŸ¥ãŒæˆåŠŸã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã®å±¥æ­´ã‚’DBã«è¨˜éŒ²
            for token, notification_hash in limited_tokens_with_hash:
                self._record_notification(
                    notification_hash,
                    token['symbol'],
                    self.timeframe,
                    token['change']['direction'],
                    token['change']['pct']
                )
            self.logger.info(f"Recorded {len(limited_tokens_with_hash)} notifications to database.")
            return True

        except requests.exceptions.RequestException as e:
            self.logger.error(f"Failed to send Discord notification: {e}")
            return False
    
    def _cleanup_old_files(self):
        """å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            current_time = datetime.now()
            
            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ7æ—¥ä»¥ä¸Šå‰ï¼‰
            cutoff_time = current_time - timedelta(days=7)
            deleted_results = 0
            
            for file_path in self.log_dir.glob('results_*.json'):
                try:
                    file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                    if file_mtime < cutoff_time:
                        file_path.unlink()
                        deleted_results += 1
                except Exception as e:
                    self.logger.warning(f"Failed to delete {file_path.name}: {e}")
            
            if deleted_results > 0:
                self.logger.info(f"Cleaned up {deleted_results} old result files")
            
            # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            total_size = sum(
                f.stat().st_size for f in self.log_dir.rglob('*') if f.is_file()
            )
            total_size_mb = total_size / (1024 * 1024)
            
            if total_size_mb > self.log_max_size_mb:
                self.logger.info(f"Log directory size ({total_size_mb:.2f}MB) exceeds limit ({self.log_max_size_mb}MB)")
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¤ã„é †ã«ã‚½ãƒ¼ãƒˆ
                files_by_age = []
                for file_path in self.log_dir.rglob('*'):
                    if file_path.is_file() and not file_path.name.endswith('.log'):  # ç¾åœ¨ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¯é™¤å¤–
                        files_by_age.append((file_path, file_path.stat().st_mtime, file_path.stat().st_size))
                
                files_by_age.sort(key=lambda x: x[1])  # å¤ã„é †
                
                deleted_size = 0
                for file_path, mtime, size in files_by_age:
                    try:
                        file_path.unlink()
                        deleted_size += size
                        self.logger.info(f"Deleted: {file_path.name} ({size/(1024*1024):.2f}MB)")
                        
                        # ç›®æ¨™ã‚µã‚¤ã‚ºä»¥ä¸‹ã«ãªã£ãŸã‚‰åœæ­¢
                        if (total_size - deleted_size) / (1024 * 1024) <= self.log_max_size_mb * 0.8:
                            break
                    except Exception as e:
                        self.logger.warning(f"Failed to delete {file_path.name}: {e}")
                
                self.logger.info(f"Log cleanup completed: {deleted_size/(1024*1024):.2f}MB freed")
        
        except Exception as e:
            self.logger.error(f"Error during cleanup: {e}")
    
    def monitor_volatility(self):
        """CMMA APIã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦é–¾å€¤ä»¥ä¸Šã®å¤‰å‹•ãŒã‚ã‚‹éŠ˜æŸ„ã‚’æ¤œå‡ºãƒ»é€šçŸ¥"""
        start_time = datetime.now()
        
        self.logger.info(f"Starting CMMA API monitoring job")
        self.logger.info(f"Settings: timeframe={self.timeframe}, threshold={self.threshold}%, direction={self.direction}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ
        self._cleanup_old_files()

        significant_moves = self.fetch_volatility_data()
        
        # å‡ºæ¥é«˜ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ãŒæœ‰åŠ¹ãªå ´åˆ
        if significant_moves and self.volume_threshold > 0:
            self.logger.info(f"Applying volume filter with threshold: {self.volume_threshold:,.2f}")
            
            # å‡ºæ¥é«˜ã®å¤§ãã„éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            high_volume_data = self.fetch_high_volume_data()

            if high_volume_data:
                filtered_moves = []
                for token in significant_moves:
                    symbol = token['symbol']
                    # å‡ºæ¥é«˜ãƒ‡ãƒ¼ã‚¿ã«ã‚·ãƒ³ãƒœãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if symbol in high_volume_data:
                        # å‡ºæ¥é«˜ã®å€¤ã‚’tokenã«è¿½åŠ 
                        token['volume'] = high_volume_data[symbol].get('total_volume')
                        filtered_moves.append(token)
                    else:
                        self.logger.info(f"Symbol {symbol} filtered out by volume. Not in high volume list.")
                
                self.logger.info(f"{len(significant_moves)} tokens -> {len(filtered_moves)} tokens after volume filter.")
                significant_moves = filtered_moves
            else:
                self.logger.warning("Could not fetch volume data or no symbols met volume threshold. All tokens will be filtered out.")
                significant_moves = [] # å‡ºæ¥é«˜ãƒ‡ãƒ¼ã‚¿ãŒå–ã‚Œãªã‹ã£ãŸã‚‰ã€ä½•ã‚‚é€šçŸ¥ã—ãªã„

        # å®Ÿè¡Œæ™‚é–“è¨ˆç®—
        execution_time = datetime.now() - start_time
        
        # çµæœã‚µãƒãƒªãƒ¼
        self.logger.info(f"Batch job completed in {execution_time.total_seconds():.1f}s")
        self.logger.info(f"Found {len(significant_moves)} tokens meeting all criteria")
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        self._save_results(significant_moves, {
            'execution_time_seconds': execution_time.total_seconds()
        })
        
        # Discordé€šçŸ¥é€ä¿¡
        if significant_moves:
            notification_sent = self.send_discord_notification(significant_moves)
            if not notification_sent:
                self.logger.warning("Discord notification failed or all tokens were filtered")
        else:
            self.logger.info("No tokens found meeting criteria")
        
        return significant_moves
    
    def _save_results(self, significant_moves, stats):
        """çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = self.log_dir / f'results_{timestamp}.json'
        
        result_data = {
            'timestamp': datetime.now().isoformat(),
            'threshold': self.threshold,
            'timeframe': self.timeframe,
            'direction': self.direction,
            'volume_threshold': self.volume_threshold,
            'total_found': len(significant_moves),
            'stats': stats,
            'tokens': significant_moves
        }
        
        try:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Results saved to {results_file}")
        except Exception as e:
            self.logger.error(f"Failed to save results: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # é–‹å§‹ãƒ­ã‚°
        print(f"CMMA Price Monitor Batch - Started at {datetime.now().isoformat()}")
        
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å–å¾—
        config_path = sys.argv[1] if len(sys.argv) > 1 else None
        
        monitor = CmmaPriceMonitor(config_path)
        
        while True:
            significant_moves = monitor.monitor_volatility()
            
            # çµæœå‡ºåŠ›
            if significant_moves:
                print(f"SUCCESS: {len(significant_moves)} tokens found meeting criteria")
                
                # ä¸Šä½5ä»¶ã‚’è¡¨ç¤º
                print("Top moving tokens:")
                for i, token in enumerate(significant_moves[:5], 1):
                    sign = "+" if token['change']['direction'] == 'up' else ""
                    print(f"  {i}. {token['symbol']}: {sign}{token['change']['pct']:.2f}%")
            else:
                print(f"INFO: No tokens found meeting criteria")
            
            print(f"Waiting for {monitor.check_interval_seconds} seconds until the next check...")
            time.sleep(monitor.check_interval_seconds)
        
    except KeyboardInterrupt:
        print("INFO: Process interrupted by user.")
        sys.exit(0)
    except Exception as e:
        print(f"ERROR: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
